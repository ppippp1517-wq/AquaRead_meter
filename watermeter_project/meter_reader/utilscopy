

import os, math, re, shutil
import cv2, numpy as np
from PIL import Image, ImageEnhance
import pytesseract
from ultralytics import YOLO
from django.conf import settings
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.gridspec import GridSpec

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ---
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á tesseract.exe
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

#  ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• 
# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• YOLO ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
MODEL_PATH = 'D:/projectCPE/dataset/runs/detect/train12/weights/best.pt'
model = YOLO(MODEL_PATH)

# 2. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏Ç‡πá‡∏°
CNN_MODEL_PATH = 'D:/projectCPE/watermeter_project/meter_reader/modelneedle.h5' 
try:
    cnn_model = tf.keras.models.load_model(CNN_MODEL_PATH)
    print(f" ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN '{CNN_MODEL_PATH}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f" ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN: {e}")
    cnn_model = None

#  ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡∏°‡πà
def detect_needle_value(image_path):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏±‡∏î‡πÄ‡∏•‡πá‡∏Å‡πÜ ‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN
    ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (final_value)
    """
    if cnn_model is None:
        print(" ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        return None

    try:
        # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏™‡∏µ 32x32)
        img = Image.open(image_path).resize((32, 32))
        image_array = np.array(img, dtype="float32")
        img_batch = np.reshape(image_array, [1, 32, 32, 3])
    except FileNotFoundError:
        print(f" ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà: {image_path}")
        return None

    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡∏∑‡∏≠ [sin, cos])
    prediction = cnn_model.predict(img_batch)
    sin_val = prediction[0][0]
    cos_val = prediction[0][1]

    # ‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡∏Ñ‡πà‡∏≤ Sin/Cos ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    value_rad = np.arctan2(sin_val, cos_val)
    value_normalized = (value_rad / (2 * math.pi)) % 1
    final_value = value_normalized * 10 # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πÄ‡∏Å‡∏• 0-10

    return final_value






# =====================================
# 1Ô∏è‚É£ WaterMeterAligner (‡∏û‡∏£‡πâ‡∏≠‡∏° save detailed_comparison & feature_matches)
# =====================================
class WaterMeterAligner:
    def __init__(self):
        self.detector = cv2.SIFT_create(nfeatures=8000)
        self.good_match_percent = 0.2

    def preprocess_meter_image(self, image):
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        return cv2.GaussianBlur(clahe.apply(gray), (3,3), 0)

    def detect_and_compute(self, image):
        return self.detector.detectAndCompute(self.preprocess_meter_image(image), None)

    def match_features(self, desc1, desc2):
        if desc1 is None or desc2 is None: return []
        matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
        matches = sorted(matcher.match(desc1, desc2), key=lambda x: x.distance)
        return matches[:int(len(matches)*self.good_match_percent)]

    def detect_circular_features(self, image):
        """‡πÉ‡∏ä‡πâ Hough Circle ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö alignment"""
        gray = self.preprocess_meter_image(image)
        return cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 50,
                                param1=50, param2=30, minRadius=50, maxRadius=300)

    def align_meter_images(self, reference_img, image_to_align):
        print("üîπ Detecting keypoints...")
        kp1, desc1 = self.detect_and_compute(reference_img)
        kp2, desc2 = self.detect_and_compute(image_to_align)
        print(f"   Reference keypoints: {len(kp1)} | Test keypoints: {len(kp2)}")

        matches = self.match_features(desc1, desc2)
        print(f"üîπ Good matches: {len(matches)}")

        if len(matches) < 4:
            print("‚ùå Not enough matches for homography")
            return image_to_align, None, len(matches), 0.0

        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)
        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)
        H, mask = cv2.findHomography(pts2, pts1, cv2.RANSAC, 3.0)
        if H is None:
            print("‚ùå Could not compute homography")
            return image_to_align, None, len(matches), 0.0

        h, w = reference_img.shape[:2]
        aligned = cv2.warpPerspective(image_to_align, H, (w,h))
        inliers = np.sum(mask)
        quality = inliers / len(matches)
        print(f"‚úÖ Alignment quality: {quality:.2f} ({inliers}/{len(matches)} inliers)")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Circular Alignment
        ref_circles = self.detect_circular_features(reference_img)
        aligned_circles = self.detect_circular_features(aligned)
        circular_error = None
        if ref_circles is not None and aligned_circles is not None:
            ref_center = (ref_circles[0][0][0], ref_circles[0][0][1])
            aligned_center = (aligned_circles[0][0][0], aligned_circles[0][0][1])
            circular_error = np.sqrt((ref_center[0]-aligned_center[0])**2 +
                                     (ref_center[1]-aligned_center[1])**2)
            print(f"üîπ Circular alignment error: {circular_error:.1f} px")
        else:
            print("‚ö†Ô∏è Circular alignment check skipped (no circle found)")

        # --- Save Alignment Results ---
        os.makedirs("alignment_results", exist_ok=True)
        aligned_path = "alignment_results/aligned_water_meter.jpg"
        cv2.imwrite(aligned_path, aligned)
        np.save("alignment_results/transformation_matrix.npy", H)
        print("‚úì Alignment results saved")

        # --- Save detailed_comparison.png ---
        self.save_detailed_comparison(reference_img, image_to_align, aligned, len(matches), quality)

        # --- Save enhanced_feature_matches.png ---
        self.save_feature_matches(reference_img, image_to_align, matches, kp1, kp2)

        return aligned, H, len(matches), quality

    def save_detailed_comparison(self, ref_img, test_img, aligned_img, match_count, quality_score):
        ref_rgb = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)
        test_rgb = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)
        aligned_rgb = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB)
        diff = cv2.absdiff(ref_rgb, aligned_rgb)
        diff_gray = cv2.cvtColor(diff, cv2.COLOR_RGB2GRAY)

        fig = plt.figure(figsize=(18,10))
        gs = GridSpec(2,3, figure=fig)
        ax1 = fig.add_subplot(gs[0,0]); ax1.imshow(ref_rgb); ax1.set_title("Reference"); ax1.axis('off')
        ax2 = fig.add_subplot(gs[0,1]); ax2.imshow(test_rgb); ax2.set_title("Original"); ax2.axis('off')
        ax3 = fig.add_subplot(gs[0,2]); ax3.imshow(aligned_rgb); ax3.set_title("Aligned"); ax3.axis('off')
        ax4 = fig.add_subplot(gs[1,0]); ax4.imshow(diff_gray, cmap='hot'); ax4.set_title("Difference"); ax4.axis('off')
        ax5 = fig.add_subplot(gs[1,1]); ax5.text(0.05,0.6,f"Matches: {match_count}\nQuality: {quality_score:.2f}",
                                                 fontsize=14,fontweight='bold'); ax5.axis('off')

        plt.tight_layout()
        path = "alignment_results/detailed_comparison.png"
        fig.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig)
        print(f"‚úì Detailed comparison saved: {path}")

    def save_feature_matches(self, ref_img, test_img, matches, kp1, kp2):
        match_img = cv2.drawMatches(ref_img, kp1, test_img, kp2,
                                    matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
        path = "alignment_results/enhanced_feature_matches.png"
        cv2.imwrite(path, match_img)
        print(f"‚úì Feature matches saved: {path}")


# =====================================
# 2Ô∏è‚É£ Crop + Rotate ‡∏î‡πâ‡∏ß‡∏¢ Hough Circle
# =====================================
def crop_and_rotate_hough(image_path, rotation_angle=-5.0):
    img = cv2.imread(image_path)
    if img is None: return None

    h, w = img.shape[:2]
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (9,9), 2)
    edges = cv2.Canny(blurred, 50, 150)

    min_r, max_r = int(min(h,w)*0.3), int(min(h,w)*0.7)
    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1.2, h//4,
                               param1=100, param2=60, minRadius=min_r, maxRadius=max_r)

    if circles is None:
        print("‚ö†Ô∏è HoughCircle ‡πÑ‡∏°‡πà‡∏û‡∏ö ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏¥‡∏°")
        return img

    x, y, r = np.round(circles[0][0]).astype(int)
    mask = np.zeros((h,w), dtype=np.uint8)
    cv2.circle(mask, (x,y), r, 255, -1)

    # Crop ROI
    top, bottom = max(y-r,0), min(y+r,h)
    left, right = max(x-r,0), min(x+r,w)
    circular_roi = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    circular_roi = np.where(mask[...,None]==255, circular_roi, 255)
    roi = circular_roi[top:bottom, left:right]
    mask_cropped = mask[top:bottom, left:right]

    # Rotate
    center = (roi.shape[1]//2, roi.shape[0]//2)
    M = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)
    rotated = cv2.warpAffine(roi, M, (roi.shape[1], roi.shape[0]),
                             flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))
    mask_rot = cv2.warpAffine(mask_cropped, M, (mask_cropped.shape[1], mask_cropped.shape[0]),
                              flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=0)
    return cv2.cvtColor(np.where(mask_rot[...,None]==255, rotated, 255), cv2.COLOR_RGB2BGR)


# =====================================
# 3Ô∏è‚É£ CNN ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πá‡∏°
# =====================================
def detect_needle_value(image_path):
    if cnn_model is None: return None
    img = Image.open(image_path).resize((32,32))
    arr = np.array(img,dtype="float32")
    batch = np.reshape(arr, [1,32,32,3])
    sin_val, cos_val = cnn_model.predict(batch)[0]
    rad = np.arctan2(sin_val, cos_val)
    return ((rad/(2*math.pi)) % 1)*10


# =====================================
# 4Ô∏è‚É£ Pipeline ‡∏´‡∏•‡∏±‡∏Å
# =====================================
def process_meter_image(image_path):
    print("\n=== WATER METER PIPELINE START ===")

    # Step 1: Crop + Rotate
    preprocessed_img = crop_and_rotate_hough(image_path, rotation_angle=-5.0)

    # Step 2: Alignment
    reference_path = "D:/projectCPE/water_meter_alignment/Reference.jpg"
    ref_img = cv2.imread(reference_path)
    test_img = cv2.resize(preprocessed_img, (750, 750))
    aligner = WaterMeterAligner()
    aligned_img, H, match_count, quality = aligner.align_meter_images(ref_img, test_img)

    # Step 3: YOLO Detection
    result = model.predict(source="alignment_results/aligned_water_meter.jpg", conf=0.25)[0]
    df = result.to_df()
    if df.empty:
        return {'digital_x':'0','x001':'0','x0001':'0','x00001':'0','total':'0.000','detected_image_path':None}

    # Step 4: CNN+OCR
    original_img = Image.fromarray(cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB))
    ocr_result_by_class = {}
    for class_id in [0,1,2,3]:
        rows = df[df['class']==class_id]
        if rows.empty:
            ocr_result_by_class[class_id] = '0'
            continue
        best = rows.loc[rows['confidence'].idxmax()]
        x1,y1,x2,y2 = int(best['box']['x1']), int(best['box']['y1']), int(best['box']['x2']), int(best['box']['y2'])
        crop = original_img.crop((x1,y1,x2,y2))
        enhance = ImageEnhance.Contrast(crop).enhance(2.0)
        temp = f'temp_crop_class{class_id}.png'
        enhance.save(temp)

        if class_id == 0:
            text = pytesseract.image_to_string(enhance, config='--psm 7 -c tessedit_char_whitelist=0123456789')
            cleaned = re.sub(r'\D','',text.strip())
            ocr_result_by_class[class_id] = cleaned if cleaned else '0'
        else:
            val = detect_needle_value(temp)
            ocr_result_by_class[class_id] = str(int(round(val))) if val else '0'
        os.remove(temp)

    int_part = ocr_result_by_class[0]
    decimal1, decimal2, decimal3 = ocr_result_by_class[1], ocr_result_by_class[2], ocr_result_by_class[3]
    combined = f"{int_part}.{decimal1}{decimal2}{decimal3}"

    # Step 5: Save detect image for Django
    yolo_path = f"D:/projectCPE/dataset/images/detect_images/detected_{os.path.basename(image_path)}"
    cv2.imwrite(yolo_path, result.plot())

    media_path = os.path.join(settings.MEDIA_ROOT, 'outputs', os.path.basename(yolo_path))
    os.makedirs(os.path.dirname(media_path), exist_ok=True)
    shutil.copy(yolo_path, media_path)

    print("\n=== ALIGNMENT SUMMARY ===")
    print(f"Feature matches: {match_count}")
    print(f"Alignment quality: {quality:.2f}/1.0")
    print("Results saved in alignment_results/")
    print("=== PIPELINE DONE ===\n")

    return {'digital_x':int_part,'x001':decimal1,'x0001':decimal2,'x00001':decimal3,
            'total':combined,'detected_image_path':f"/media/outputs/{os.path.basename(yolo_path)}"}



####‡∏≠‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
# -*- coding: utf-8 -*-

import os, math, re, shutil, glob
import cv2, numpy as np
from PIL import Image, ImageEnhance, ImageDraw, ImageFont
import pytesseract
from ultralytics import YOLO
from django.conf import settings
import tensorflow as tf
from .models import Meter, MeterReading
import warnings

# ================= Output folders =================

CAPTURE_DIR = r"D:/projectCPE/dataset/images/capture_images"
DETECT_DIR  = r"D:/projectCPE/dataset/images/detect_images"
CROP_DIR    = r"D:/projectCPE/dataset/images/cropped_images"
CROP_CLASS0_DIR = os.path.join(CROP_DIR, "class0")         # (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏î 1 box ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏ã‡∏ü strip ‡πÑ‡∏î‡πâ)
CROP_CLASSX_DIR = os.path.join(CROP_DIR, "class1-3")
DIGIT_CROPS_DIR = os.path.join(CROP_CLASS0_DIR, "digits")  # 20x32 ‡∏ï‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å

SUMMARY_DIR      = r"D:/projectCPE/dataset/images/summary_digit"     # ‡∏†‡∏≤‡∏û‡∏™‡∏£‡∏∏‡∏õ‡∏ù‡∏±‡πà‡∏á‡∏Ç‡∏ß‡∏≤
DETECT_DIGIT_DIR = r"D:/projectCPE/dataset/images/detect_digit"      # ‡∏†‡∏≤‡∏û detect ‡∏Ç‡∏≠‡∏á YOLO ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

for d in [CAPTURE_DIR, DETECT_DIR, CROP_DIR, CROP_CLASS0_DIR, CROP_CLASSX_DIR,
          DIGIT_CROPS_DIR, SUMMARY_DIR, DETECT_DIGIT_DIR]:
    os.makedirs(d, exist_ok=True)

# =============== OPTIONAL DEPENDENCIES ===============

try:
    import cv2.ximgproc as xip
    _HAS_XIMGPROC = True
except Exception:
    _HAS_XIMGPROC = False

try:
    from skimage.morphology import skeletonize
    _HAS_SKIMAGE = True
except Exception:
    _HAS_SKIMAGE = False

warnings.filterwarnings("ignore", category=FutureWarning)

# =============== Models ===============

pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# YOLO ‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏•‡∏≤‡∏™ (0 = digital strip, 1..3 = ‡πÄ‡∏Ç‡πá‡∏°) ‚Äî ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 1..3 ‡πÅ‡∏•‡πâ‡∏ß
YOLO_MULTI_PATH = 'D:/projectCPE/dataset/runs/detect/train12/weights/best.pt'
model_multi = YOLO(YOLO_MULTI_PATH)

# YOLO ‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‚Äúdigit‚Äù
YOLO_DIGIT_PATH = r'D:/projectCPE/dataset_digital/runs/detect/digital_det2/weights/best.pt'
model_digit = YOLO(YOLO_DIGIT_PATH)

# CNN ‡πÄ‡∏Ç‡πá‡∏° (sin,cos)
CNN_MODEL_PATH = 'D:/projectCPE/watermeter_project/meter_reader/modelneedle.h5'
try:
    cnn_model = tf.keras.models.load_model(CNN_MODEL_PATH)
    print(f" ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN ‡πÄ‡∏Ç‡πá‡∏° '{CNN_MODEL_PATH}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f" ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πá‡∏°: {e}")
    cnn_model = None

# CNN ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• (0..9 + NaN(=10))
DIGIT_MODEL_PATH = r"D:/projectCPE/Train_CNN_Digital-Readout_Version_5.0.0.h5"
PAIR_MODEL_PATH  = r"D:/projectCPE/pair_ab_keras.h5"
try:
    digit_model = tf.keras.models.load_model(DIGIT_MODEL_PATH)
    pair_model  = tf.keras.models.load_model(PAIR_MODEL_PATH, compile=False)
    print(f" ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• '{DIGIT_MODEL_PATH}' ‡πÅ‡∏•‡∏∞ pair '{PAIR_MODEL_PATH}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f" ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•: {e}")
    digit_model, pair_model = None, None

# =============== Spec & preprocess ===============

def _get_digit_model_spec():
    if digit_model is None:
        # fallback ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö 20x32 (H=20, W=32)
        return 20, 32, 3, "RGB"
    _, H, W, C = digit_model.input_shape
    color = "L" if C == 1 else "RGB"
    return int(H), int(W), int(C), color

def preprocess_for_digit_model(pil_img: Image.Image):
    H, W, C, color = _get_digit_model_spec()
    img = pil_img.convert(color).resize((W, H), Image.BILINEAR)
    arr = np.array(img, dtype=np.float32)
    if C == 1 and arr.ndim == 2:
        arr = np.expand_dims(arr, -1)
    elif C == 3 and arr.ndim == 2:
        arr = np.stack([arr, arr, arr], axis=-1)
    # NOTE: ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ö‡∏ö ‚Äú‡πÑ‡∏°‡πà‡∏´‡∏≤‡∏£ 255‚Äù -> ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô False ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    NORMALIZE = False
    if NORMALIZE:
        arr = arr/255.0
    return np.expand_dims(arr, 0)

# =============== Digit enhancement & rules ===============

DO_CLAHE   = True
DO_UNSHARP = True
GAMMA      = 0.95

DIGIT_CONF_MIN = 0.80
NAN_CONF_MIN   = 0.50
PAIR_CONF_MIN  = 0.50
MID_RATIO      = 0.50
AREA_THR       = 0.30
PAIR_THR       = {}        # ex. {6:0.68, 7:0.62}
IGNORE_X_MARGIN     = 0.10
STRONG_PAIR_CONF    = 0.80      # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏¢‡∏≠‡∏°‡∏î‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡∏Ç‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
EXTRA_BELOW_MARGIN  = 0.10

def get_or_create_meter(meter_id: str) -> Meter:
    obj, _ = Meter.objects.get_or_create(meter_id=meter_id)
    return obj

def get_prev_reading_digits(meter: Meter):
    last = MeterReading.objects.filter(meter=meter).order_by("-timestamp").first()
    if not last:
        return None
    return {
        "x01":    int(last.x01),
        "x001":   int(last.x001),
        "x0001":  int(last.x0001),
        "x00001": int(last.x00001),
    }

# Summary look
SRC_SCALE          = 0.60
DIGIT_THUMB_H      = 110
DIGIT_SPACING      = 48
STRIP_LEFT_OFFSET  = 120
TOTAL_OFFSET_UP    = 18
FONT_DIGIT_PATH    = r'C:/Windows/Fonts/arial.ttf'
FONT_TOTAL_PATH    = r'C:/Windows/Fonts/arialbd.ttf'

def enhance_digit(pil_img: Image.Image) -> Image.Image:
    gray = np.array(pil_img.convert('L'))
    if DO_CLAHE:
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))
        gray = clahe.apply(gray)
    if DO_UNSHARP:
        blur = cv2.GaussianBlur(gray, (0,0), 1.0)
        gray = cv2.addWeighted(gray, 1.4, blur, -0.4, 0)
    if abs(GAMMA - 1.0) > 1e-6:
        inv = 1.0 / GAMMA
        table = (np.linspace(0,1,256) ** inv) * 255.0
        table = np.clip(table, 0, 255).astype(np.uint8)
        gray  = table[gray]
    rgb = np.dstack([gray, gray, gray])
    return Image.fromarray(rgb).convert("RGB")

def to_dark_mask(g_uint8):
    _, mask = cv2.threshold(g_uint8, 0, 1, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)
    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, k)
    return mask.astype(np.float32)

def dark_fraction_below(mask, mid_ratio=0.5, ignore_margin=0.10):
    H, W = mask.shape
    y_mid = int(round(mid_ratio*(H-1)))
    x0 = int(round(ignore_margin*W)); x1 = W - x0
    if x1 <= x0: x0, x1 = 0, W
    roi = mask[:, x0:x1]
    tot = roi.sum() + 1e-6
    below = roi[y_mid+1:, :].sum()
    return float(below/tot), y_mid

def text_size(draw, text, font):
    if hasattr(font, "getbbox"):
        l,t,r,b = font.getbbox(text)
        return r-l, b-t
    return draw.textsize(text, font=font)

# =============== Analog (needle) ===============

def detect_needle_value(image_path):
    if cnn_model is None:
        return None
    img = Image.open(image_path).resize((32,32)).convert("RGB")
    arr = np.array(img, dtype=np.float32)
    batch = np.reshape(arr, [1,32,32,3])
    sin_val, cos_val = cnn_model.predict(batch, verbose=0)[0]
    rad = np.arctan2(sin_val, cos_val)
    return ((rad/(2*math.pi)) % 1) * 10

# =============== Alignment ===============

class WaterMeterAligner:
    def __init__(self):
        self.detector = cv2.SIFT_create(nfeatures=8000)
        self.good_match_percent = 0.2
    def preprocess_meter_image(self, image):
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        return cv2.GaussianBlur(clahe.apply(gray), (3,3), 0)
    def detect_and_compute(self, image):
        return self.detector.detectAndCompute(self.preprocess_meter_image(image), None)
    def match_features(self, desc1, desc2):
        if desc1 is None or desc2 is None:
            return []
        matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
        matches = sorted(matcher.match(desc1, desc2), key=lambda x: x.distance)
        return matches[:int(len(matches)*self.good_match_percent)]
    def detect_circular_features(self, image):
        gray = self.preprocess_meter_image(image)
        return cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 50, param1=50, param2=30, minRadius=50, maxRadius=300)
    def align_meter_images(self, reference_img, image_to_align):
        kp1, desc1 = self.detect_and_compute(reference_img)
        kp2, desc2 = self.detect_and_compute(image_to_align)
        matches = self.match_features(desc1, desc2)
        if len(matches) < 4:
            return image_to_align, None, len(matches), 0.0
        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)
        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)
        H, mask = cv2.findHomography(pts2, pts1, cv2.RANSAC, 3.0)
        if H is None:
            return image_to_align, None, len(matches), 0.0
        h, w = reference_img.shape[:2]
        aligned = cv2.warpPerspective(image_to_align, H, (w,h))
        inliers = int(np.sum(mask)) if mask is not None else 0
        quality = inliers / max(1, len(matches))
        os.makedirs("alignment_results", exist_ok=True)
        aligned_path = "alignment_results/aligned_water_meter.jpg"
        cv2.imwrite(aligned_path, aligned)
        np.save("alignment_results/transformation_matrix.npy", H)
        return aligned, H, len(matches), quality

# =============== Crop+Rotate (Hough circle) ===============

def crop_and_rotate_hough(image_path, rotation_angle=-5.0):
    img = cv2.imread(image_path)
    if img is None: return None
    h, w = img.shape[:2]
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(cv2.GaussianBlur(gray, (9,9), 2), 50, 150)
    min_r, max_r = int(min(h,w)*0.3), int(min(h,w)*0.7)
    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1.2, h//4,
                               param1=100, param2=60, minRadius=min_r, maxRadius=max_r)
    if circles is None:
        return img
    x, y, r = np.round(circles[0][0]).astype(int)
    mask = np.zeros((h,w), dtype=np.uint8); cv2.circle(mask, (x,y), r, 255, -1)
    top, bottom = max(y-r,0), min(y+r,h); left, right = max(x-r,0), min(x+r,w)
    circular_roi = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    circular_roi = np.where(mask[...,None]==255, circular_roi, 255)
    roi = circular_roi[top:bottom, left:right]
    mask_cropped = mask[top:bottom, left:right]
    center = (roi.shape[1]//2, roi.shape[0]//2)
    M = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)
    rotated = cv2.warpAffine(roi, M, (roi.shape[1], roi.shape[0]),
                             flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))
    mask_rot = cv2.warpAffine(mask_cropped, M, (mask_cropped.shape[1], mask_cropped.shape[0]),
                              flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT, borderValue=0)
    return cv2.cvtColor(np.where(mask_rot[...,None]==255, rotated, 255), cv2.COLOR_RGB2BGR)

# ====== Carry: ‡πÑ‡∏•‡πà‡∏à‡∏≤‡∏Å‡∏Ç‡∏ß‡∏≤ ‚Üí ‡∏ã‡πâ‡∏≤‡∏¢ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏£‡∏¥‡∏á ======

def _wrapped_simple(prev_digit, curr_digit):
    """‡∏´‡∏≤‡∏ß‡πà‡∏≤ wrap 9‚Üí0 ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ prev ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ä‡∏±‡∏ß‡∏£‡πå)"""
    if prev_digit is None:
        return False
    return (prev_digit == 9 and int(curr_digit) == 0)

def _inc_digit(d):
    d = int(d) + 1
    carry = (d == 10)
    return (0 if carry else d), (1 if carry else 0)

def cascade_carry(integer, x01, x001, x0001, x00001, prev=None):
    """
    integer: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (string ‡∏´‡∏£‡∏∑‡∏≠ int ‡πÑ‡∏î‡πâ)
    x01..x00001: ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å (string/int)
    prev: dict {'x01':..,'x001':..,'x0001':..,'x00001':..} ‡∏´‡∏£‡∏∑‡∏≠ None
    """
    I  = int(integer) if isinstance(integer, str) else integer
    d1 = int(x01); d2 = int(x001); d3 = int(x0001); d4 = int(x00001)

    pv = prev or {}

    # x00001 ‚Üí x0001
    if _wrapped_simple(pv.get('x00001'), d4):
        d3, c = _inc_digit(d3)
        if c:
            d2, c = _inc_digit(d2)
            if c:
                d1, c = _inc_digit(d1)
                if c:
                    I += 1

    # x0001 ‚Üí x001
    if _wrapped_simple(pv.get('x0001'), d3):
        d2, c = _inc_digit(d2)
        if c:
            d1, c = _inc_digit(d1)
            if c:
                I += 1

    # x001 ‚Üí x01
    if _wrapped_simple(pv.get('x001'), d2):
        d1, c = _inc_digit(d1)
        if c:
            I += 1

    # x01 ‚Üí integer
    if _wrapped_simple(pv.get('x01'), d1):
        I += 1

    return I, d1, d2, d3, d4

# =========================================================
# ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏• (‡πÅ‡∏ó‡∏ô class0 ‡πÄ‡∏î‡∏¥‡∏°): YOLO single-class + CNN + summary
# =========================================================

def run_digital_reader_on_aligned(aligned_bgr: np.ndarray, stem: str):
    """
    aligned_bgr: ‡∏†‡∏≤‡∏û‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà align ‡πÅ‡∏•‡πâ‡∏ß (BGR)
    stem: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ê‡∏≤‡∏ô (‡πÑ‡∏°‡πà‡πÄ‡∏≠‡∏≤‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏ü
    return: (string_5digits, detect_path, summary_path)
    """
    if digit_model is None or pair_model is None:
        return "00000", None, None

    # 1) Detect digits (single-class YOLO) ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û align
    rs = model_digit.predict(source=aligned_bgr, imgsz=832, conf=0.05, iou=0.5, agnostic_nms=False, verbose=False)
    r = rs[0]
    if r.boxes is None or len(r.boxes) == 0:
        return "00000", None, None

    # save detect image
    det_bgr = r.plot()
    det_path = os.path.join(DETECT_DIGIT_DIR, f"{stem}_digit_detect.jpg")
    cv2.imwrite(det_path, det_bgr)

    # sort boxes left->right
    boxes = r.boxes.xyxy.cpu().numpy()
    order = np.argsort(boxes[:,0])
    boxes = boxes[order]

    pil_src = Image.fromarray(cv2.cvtColor(aligned_bgr, cv2.COLOR_BGR2RGB))
    Hm, Wm, Cm = digit_model.input_shape[1:4]
    Hp, Wp, Cp = pair_model.input_shape[1:4]

    preds = []
    crops_display = []
    paths_20x32   = []
    RESIZE_WH     = (20, 32)

    for i,(x1,y1,x2,y2) in enumerate(boxes, start=1):
        x1,y1,x2,y2 = map(int, (x1,y1,x2,y2))
        crop = pil_src.crop((x1,y1,x2,y2))
        base = f"{stem}_{i}"

        # ‡πÄ‡∏ã‡∏ü 20x32 (‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á)
        crop_20x32 = crop.resize(RESIZE_WH, Image.BILINEAR)
        p_20x32 = os.path.join(DIGIT_CROPS_DIR, base + "_20x32.png")
        crop_20x32.save(p_20x32)
        crops_display.append(crop_20x32)
        paths_20x32.append(p_20x32)

        # 2) predict main + pair
        pil_enh = enhance_digit(crop_20x32)
        arr = np.array(pil_enh.resize((Wm, Hm), Image.BILINEAR), dtype="float32")
        if Cm == 1:
            arr = cv2.cvtColor(arr.astype(np.uint8), cv2.COLOR_RGB2GRAY).astype(np.float32)[...,None]
        # NORMALIZE False ‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ñ‡∏∏‡∏ì
        probs = digit_model.predict(arr[None,...], verbose=0)[0]
        cls = int(np.argmax(probs)); digit_conf = float(probs[cls])
        nan_idx  = 10 if probs.shape[0] == 11 else None
        nan_conf = float(probs[nan_idx]) if nan_idx is not None else 0.0
        is_nan   = (nan_idx is not None and cls == nan_idx)

        if (not is_nan) and (nan_conf < NAN_CONF_MIN or digit_conf >= DIGIT_CONF_MIN):
            final = cls
        else:
            g_full = np.array(pil_enh.convert("L"))
            g_resz = cv2.resize(g_full, (Wp, Hp), interpolation=cv2.INTER_LINEAR)
            xpair  = (g_resz.astype(np.float32)/255.0)
            if Cp == 1: xpair = xpair[...,None]
            else:       xpair = np.stack([xpair,xpair,xpair], axis=-1)
            pr = pair_model.predict(xpair[None,...], verbose=0)[0]
            a = int(pr.argmax()); b = (a + 1) % 10
            conf = float(pr[a])

            mask = to_dark_mask(g_resz)
            frac, _ = dark_fraction_below(mask, MID_RATIO, IGNORE_X_MARGIN)
            thr_used = PAIR_THR.get(a, AREA_THR)

            # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 'a' (‡πÄ‡∏•‡∏Ç‡πÄ‡∏î‡∏¥‡∏°/‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤)
            final = a

            # ‡∏à‡∏∞‡∏¢‡∏≠‡∏° "‡∏î‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô b" ‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏ê‡∏≤‡∏ô‡πÅ‡∏£‡∏á‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:
            # 1) pair ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å ‡πÅ‡∏•‡∏∞
            # 2) ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á (below) ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ threshold + margin
            if (conf >= STRONG_PAIR_CONF) and (frac >= (thr_used + EXTRA_BELOW_MARGIN)):
                final = b

        preds.append(final)

    # 3) ‡∏™‡∏£‡πâ‡∏≤‡∏á summary (‡∏ã‡πâ‡∏≤‡∏¢: detected, ‡∏Ç‡∏ß‡∏≤: ‡πÅ‡∏ñ‡∏ß‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç + ‡∏Å‡∏£‡∏≠‡∏ö‡∏£‡∏ß‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô)
    # scale left
    W0,H0 = pil_src.size
    W = int(W0 * SRC_SCALE); H = int(H0 * SRC_SCALE)
    pil_left = Image.fromarray(cv2.cvtColor(det_bgr, cv2.COLOR_BGR2RGB)).resize((W,H), Image.LANCZOS)

    # thumbs
    thumbs = []
    for t in crops_display:
        th = DIGIT_THUMB_H
        tw = int(round(t.width * (th / t.height)))
        thumbs.append(t.resize((tw, th), Image.NEAREST))
    strip_w = sum(t.width for t in thumbs) + DIGIT_SPACING*(len(thumbs)-1) if thumbs else 0
    strip_h = DIGIT_THUMB_H

    canvas_w = max(W + STRIP_LEFT_OFFSET + strip_w + 60, W + 600)
    canvas_h = max(H, strip_h + 120)
    canvas = Image.new("RGB", (canvas_w, canvas_h), (245,245,245))
    canvas.paste(pil_left, (0, (canvas_h - H)//2))
    d = ImageDraw.Draw(canvas)
    font_digit = ImageFont.truetype(FONT_DIGIT_PATH, 22)
    font_total = ImageFont.truetype(FONT_TOTAL_PATH, 40)

    # total box (‡∏ö‡∏ô‡πÅ‡∏ñ‡∏ß)
    total_text = ''.join(str(x) for x in preds) if preds else '-----'
    tw, th = text_size(d, total_text, font_total)
    row_x = W + STRIP_LEFT_OFFSET
    row_y = (canvas_h - strip_h)//2
    bx = row_x + (strip_w // 2) - (tw // 2)
    by = row_y - (th + 26) - TOTAL_OFFSET_UP
    d.rectangle([bx-12, by-12, bx+tw+12, by+th+12], outline=(20,80,200), width=3)
    d.text((bx, by), total_text, font=font_total, fill=(10,10,10))

    # row
    x = row_x; y = row_y
    for i, t in enumerate(thumbs):
        canvas.paste(t, (x, y))
        lbl = str(preds[i]) if i < len(preds) else '?'
        ltw, lth = text_size(d, lbl, font_digit)
        d.rectangle([x, y- lth - 10, x + ltw + 10, y], fill=(0,0,0))
        d.text((x+5, y - lth - 6), lbl, font=font_digit, fill=(255,255,255))
        x += t.width + DIGIT_SPACING

    summary_path = os.path.join(SUMMARY_DIR, f"{stem}_summary.jpg")
    canvas.save(summary_path, quality=92)

    return total_text, det_path, summary_path

# =========================================================
# Main pipeline
# =========================================================

def process_meter_image(image_path, meter_id="DEFAULT"):
    base_name = os.path.basename(image_path)
    captured_path = os.path.join(CAPTURE_DIR, base_name)
    try:
        if os.path.isfile(image_path):
            shutil.copy(image_path, captured_path)
    except Exception as e:
        print("[WARN] cannot save uploaded image:", e)

    print("\n=== WATER METER PIPELINE (NO CROP/ALIGN) START ===")

    # 0) ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö BGR (‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞‡∏ï‡πâ‡∏≠‡∏á ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏õ ‡πÑ‡∏°‡πà‡∏´‡∏°‡∏∏‡∏ô)
    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        print("[ERROR] Cannot read image:", image_path)
        return {
            'stem'      : os.path.splitext(base_name)[0],
            'digital_x' : "0",
            'x01'       : '0', 'x001': '0', 'x0001': '0', 'x00001': '0',
            'total'     : "0.0000",
            'detected_image_path': None,
            'digit_detect_path'  : None,
            'digit_summary_path' : None
        }
    stem = os.path.splitext(base_name)[0]

    # 1) YOLO multi-class (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ class 1..3 ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà conf ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î) ‚Äî ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    result_multi = model_multi.predict(source=img_bgr, conf=0.25, verbose=False)[0]

    # ‡πÄ‡∏ã‡∏ü‡∏†‡∏≤‡∏û detect ‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á multi-class
    yolo_path = os.path.join(DETECT_DIR, f"detected_{base_name}")
    cv2.imwrite(yolo_path, result_multi.plot())

    # 2) ‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà: ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏î‡∏¥‡∏° ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ single-class YOLO + CNN
    #    (‡∏ä‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô run_digital_reader_on_aligned ‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û‡∏î‡∏¥‡∏ö‡πÅ‡∏ó‡∏ô)
    int_part, digit_det_path, digit_sum_path = run_digital_reader_on_aligned(img_bgr, stem)
    if not int_part:
        int_part = "00000"

    # 3) ‡πÄ‡∏Ç‡πá‡∏° class1..3: ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å result_multi
    original_img_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))
    ocr_result_by_class = {}

    if result_multi.boxes is None or len(result_multi.boxes) == 0:
        # ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏• detect ‡πÄ‡∏•‡∏¢ ‚Üí ‡πÉ‡∏™‡πà 0 ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≤‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á
        for class_id in [1,2,3]:
            ocr_result_by_class[class_id] = '0'
    else:
        xyxy = result_multi.boxes.xyxy.cpu().numpy().astype(int)
        cls_arr = result_multi.boxes.cls.cpu().numpy().astype(int)
        confs  = result_multi.boxes.conf.cpu().numpy()

        for class_id in [1,2,3]:
            idxs = np.where(cls_arr == class_id)[0]
            if idxs.size == 0:
                ocr_result_by_class[class_id] = '0'
                continue
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏ó‡∏µ‡πà conf ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ô‡∏µ‡πâ
            best_local = idxs[np.argmax(confs[idxs])]
            x1, y1, x2, y2 = xyxy[best_local]

            # ‡∏Ñ‡∏£‡∏≠‡∏õ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏ô‡∏ó‡∏£‡∏≤‡∏™‡∏ï‡πå‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πá‡∏°
            crop = original_img_pil.crop((x1, y1, x2, y2)).convert("RGB")
            enhance = ImageEnhance.Contrast(crop).enhance(2.0)

            # ‡πÄ‡∏ã‡∏ü‡∏Ñ‡∏£‡∏≠‡∏õ‡πÑ‡∏ß‡πâ‡∏î‡∏π‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á
            save_path = os.path.join(CROP_CLASSX_DIR, f"{stem}_class{class_id}.png")
            enhance.save(save_path)

            # ‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏Ç‡πá‡∏°‡πÄ‡∏î‡∏¥‡∏° (‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå temp ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
            temp = f'temp_crop_class{class_id}.png'
            enhance.save(temp)
            val = detect_needle_value(temp)
            ocr_result_by_class[class_id] = str(int(round(val))) if val is not None else '0'
            try:
                os.remove(temp)
            except:
                pass

    # ===== ‡πÉ‡∏´‡∏°‡πà: ‡πÅ‡∏¢‡∏Å‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° + ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° (pos1..4) =====
    # int_part = string ‡∏´‡∏•‡∏±‡∏Å‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡∏à‡∏≤‡∏Å YOLO (‡∏ã‡πâ‡∏≤‡∏¢‚Üí‡∏Ç‡∏ß‡∏≤)
    # ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏ß‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏°‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà 1 (decimal0), ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°
    int_str   = int_part or ""
    decimal0  = int_str[-1] if len(int_str) >= 1 else '0'          # pos1
    int_only  = int_str[:-1] if len(int_str) > 1 else '0'          # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏°

    # ‡πÄ‡∏Ç‡πá‡∏° 3 ‡∏ï‡∏±‡∏ß ‚Üí ‡∏ó‡∏®‡∏ô‡∏¥‡∏¢‡∏° pos2, pos3, pos4
    decimal1 = ocr_result_by_class.get(1, '0')  # pos2
    decimal2 = ocr_result_by_class.get(2, '0')  # pos3
    decimal3 = ocr_result_by_class.get(3, '0')  # pos4

    # ==== ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° prev ‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞ apply carry ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏£‡∏¥‡∏á ====
    meter_obj = get_or_create_meter(meter_id)
    prev_reading = get_prev_reading_digits(meter_obj)  # None ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥

    I_after, d1_after, d2_after, d3_after, d4_after = cascade_carry(
        integer=(int_only if int_only.lstrip('0') != '' else '0'),
        x01=decimal0, x001=decimal1, x0001=decimal2, x00001=decimal3,
        prev=prev_reading
    )

    int_only_str = str(I_after)
    decimal0 = str(d1_after); decimal1 = str(d2_after); decimal2 = str(d3_after); decimal3 = str(d4_after)

    combined = f"{int_only_str}.{decimal0}{decimal1}{decimal2}{decimal3}"

    # ===== ‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ MEDIA ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏õ‡πá‡∏ô URL =====
    def _to_media(src_path: str):
        try:
            if not src_path or not os.path.isfile(src_path):
                return None
            out_dir = os.path.join(settings.MEDIA_ROOT, 'outputs')
            os.makedirs(out_dir, exist_ok=True)
            dst = os.path.join(out_dir, os.path.basename(src_path))
            if src_path != dst:
                shutil.copy(src_path, dst)
            return f"/media/outputs/{os.path.basename(src_path)}"
        except Exception as e:
            print("[WARN] copy to media failed:", e)
            return None

    # ‡∏†‡∏≤‡∏û overlay ‡∏Ç‡∏≠‡∏á multi-class, digit detect ‡πÅ‡∏•‡∏∞ summary
    detected_url       = _to_media(yolo_path)         # result_multi.plot() ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏ü‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
    digit_detect_url   = _to_media(digit_det_path)    # ‡∏à‡∏≤‡∏Å run_digital_reader_on_aligned(...)
    digit_summary_url  = _to_media(digit_sum_path)    # ‡∏à‡∏≤‡∏Å run_digital_reader_on_aligned(...)

    print("=== PIPELINE DONE (NO CROP/ALIGN) ===\n")

    # ==== ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á History ====
    try:
        MeterReading.objects.create(
            meter=meter_obj,
            digital_x=int_only_str,
            x01=int(decimal0), x001=int(decimal1), x0001=int(decimal2), x00001=int(decimal3),
            total=combined,
            detected_image_path=(detected_url or ""),
            digit_detect_path=(digit_detect_url or ""),
            digit_summary_path=(digit_summary_url or ""),
        )
    except Exception as e:
        print("[WARN] save reading failed:", e)

    return {
        'stem'      : stem,
        'digital_x' : int_only_str,   # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ï‡πá‡∏° (‡∏´‡∏•‡∏±‡∏á carry)
        'x01'       : decimal0,
        'x001'      : decimal1,
        'x0001'     : decimal2,
        'x00001'    : decimal3,
        'total'     : combined,
        'detected_image_path' : detected_url,
        'digit_detect_path'   : digit_detect_url,
        'digit_summary_path'  : digit_summary_url
    }
